---
title: "**Homework 3**"
author: "Santiago Ruiz, Nikita Karetnikov, Felix Ubl "
output:
  pdf_document:
    latex_engine: xelatex 
date: "`r Sys.Date()`"
---

## Exersice 1

In this exercise we are asked to write a function for mixed preuso-random
number generator, using this formula:

\[
x_{n+1} = (a x_n + b) \texttt{\%\%}   M, \quad y_{n+1} = \frac{x_{n+1}}{M}
\]

Okay let us implement it in R:

```{r 1}

PRNG_generator_1 <- function(a,b,M, seed = 23){
  
  n <- 10000
  
  x <- rep(0, n)
  y <- rep(0, n)
  
  x[1] <- seed
 
  for (i in 2:n) {
    
    x[i] <- (a * x[i - 1] + b) %% M
    y[i] <- x[i] / M
  
    }

  return( (y)[-1])
}


```
Now let us check it for different values of A:
```{r 2}

y <- PRNG_generator_1(a = 205, b = 29573, M = 139968)
hist(y, breaks = 50, main = "Histogram of Mixed PRNG with a = 205", xlab = "y")

y <- PRNG_generator_1(a = 204, b = 29573, M = 139968)
hist(y, breaks = 50, main = "Histogram of Mixed PRNG with a = 204", xlab = "y")

```

We can see that for $a = 205$, our generator performs well — the distribution looks close to uniform. But in the case of $a = 204$, the generator almost constantly produces the value `0.6393533`. So basically, it loses its randomness.

After doing some research online, we found that this happens because it violates the conditions of [Hull and Dobell's theorem](https://www.howardrudd.net/mathematics/hull-and-dobells-first-theorem/), which are required for a full-period linear congruential generator (our mixed pseudo-random number generator is an example of this type).



## Exersice 2

In this exercise we are asked o generate standard normal pseudorandom numbers
using the rejection method.


We use the general formula for the acceptance probability in rejection sampling:

$$
\alpha(Y) = \frac{1}{M} \cdot \frac{f_X(Y)}{f_Y(Y)}.
$$
where \( f_X(Y) = \frac{1}{\sqrt{2\pi}} e^{-Y^2 / 2} \)  - is the PDF of the standard normal distribution, and
\( f_Y(Y) = e^{-Y} \) — is the PDF of the Exponential(1) distribution. Substituting these into the formula gives:

$$
\alpha(Y) = \frac{1}{M} \cdot \frac{\frac{1}{\sqrt{2\pi}} e^{-Y^2 / 2}}{e^{-Y}} = \frac{1}{M\sqrt{2\pi}} \cdot e^{-(Y^2 / 2 - Y)}.
$$
Simplifying the exponent:

$$
Y^2 / 2 - Y = \frac{(Y - 1)^2}{2} - \frac{1}{2},
$$

so the acceptance probability becomes:

$$
\alpha(Y) = \frac{e^{1/2}}{M\sqrt{2\pi}} \cdot \exp\left(-\frac{(Y - 1)^2}{2}\right).
$$
We ignore the constant factor \( \frac{e^{1/2}}{M\sqrt{2\pi}} \), because rejection sampling only depends on the relative acceptance probability. Hence we need to use:

$$
\alpha(Y) \propto \exp\left(-\frac{(Y - 1)^2}{2}\right).
$$
Now we can implement the rejection algorithm in R:

```{r 3}

rejection_sampling <- function(n) {
  s <- numeric(n)
  i <- 1
  
  while (i <= n) {
    y <- rexp(1)  
    u <- runif(1) 
    
    if (u <= exp(-((y - 1)^2) / 2)) {
      sign <- ifelse(runif(1) < 0.5, -1, 1)  
      s[i] <- sign * y
      i <- i + 1
    }
  }
  
  return(s)
}

```

Now, let us to use our rejection sampling algorithm to generate standard normal
distribution and plot it:

```{r 4}


hist(rejection_sampling(1000), probability = TRUE, breaks = 50, col = "lightblue",
     main = "Standard Normal Distribution via Rejection Sampling", xlab = "x")
curve(dnorm(x), col = "red", lwd = 2, add = TRUE)


```
Looks close enough to a standard normal distribution!


## Exersice 3
In this exercise we are asked to generate pseudorandom numbers
from a truncated exponential distribution. To solve this exercise we need to know two things. First one is theory on
inversion method. 

Let us use a bit of theory from the
slides. From slide 6–7, the inversion method works as follows:

If \( X \) has CDF \( F_X(x) \), and \( U \sim \text{Uniform}(0, 1) \),

then:
\[
X = F_X^{-1}(U)
\]

follows the desired distribution.

The example on Slide 7 shows the exponential distribution:

\[
F(x; \lambda) = 1 - e^{-\lambda x} \quad \Rightarrow \quad F^{-1}(u) = -\frac{1}{\lambda} \log(1 - u)
\]

---
 
Second thing we need to know is theory on truncated distribution. When the exponential distribution is truncated to the interval \([a, b]\), the CDF becomes:

\[
F_{\text{trunc}}(x) = \frac{F(x) - F(a)}{F(b) - F(a)} = \frac{e^{-\lambda a} - e^{-\lambda x}}{e^{-\lambda a} - e^{-\lambda b}}
\]

Setting \( F_{\text{trunc}}(x) = U \), and solving for \( x \), we obtain the inverse CDF:

\[
x = -\frac{1}{\lambda} \log\left( e^{-\lambda a} - U \cdot (e^{-\lambda a} - e^{-\lambda b}) \right)
\]

This is the formula we will use to generate samples.

 
 We  implement it in R:


```{r 5}

truncated_exp <- function(n, lambda, a, b) {
  u <- runif(n)
  x <- -log(exp(-lambda * a) - u * (exp(-lambda * a) - exp(-lambda * b))) / lambda
  return(x)
}
```

and generate samples using it:

```{r 6}

samples <- truncated_exp(n = 10000,lambda = 1, a = 1, b = 2.5)

hist(samples, probability = TRUE,  main = "Truncated Exponential [1, 2.5]", col = "lightblue", xlab = "x")

curve(dexp(x, rate = 1) / (pexp(2.5, rate = 1) - pexp(1, rate = 1)),
      from = 1, to = 2.5, col = "red", lwd = 2, add = TRUE)

legend("topright", legend = c("Sampled", "True PDF"),
       fill = c("lightblue", NA), border = c("white", NA),
       lty = c(NA, 1), col = c(NA, "red"), lwd = 2)

```
Looks nice -- the histrogram of generated samples is close to true PDF.

## Exercise 4   Binomial $(m,\pi)$ with **`runif()`** only

We build three generators that rely *exclusively* on uniform random numbers, give the short analytical proof that
justifies the “single-uniform chain”, and then check both accuracy and speed.

---

### 4.0 Why the “chain” works

Let $U\sim\text{U}(0,1)$ and $0<\pi<1$.

*If $U\le\pi$*, then $U$ is uniform on $[0,\pi]$.  Scaling that
interval to length 1 gives

$$
\frac{U}{\pi}\;\Bigm|\;U\le\pi \;\sim\; \text{U}(0,1).
$$

*If $U>\pi$*, then $U$ is uniform on $[\pi,1]$.  Shifting the interval down by $\pi$ and rescaling it gives

$$
\frac{U-\pi}{1-\pi}\;\Bigm|\;U>\pi \;\sim\; \text{U}(0,1).
$$

Those two elementary facts let us recycle *one and the same* uniform
number through $m$ Bernoulli trials, which is exactly what the “chain” algorithm does.

---

### 4.1 Functions

```{r 7}
## (1) inversion -----------------------------------------------------------
rbinom_inv <- local({
  cache <- list(k = NULL, cdf = NULL)          # tiny speed-up
  function(n, size, prob){
    if(size == 0L || prob == 0) return(integer(n))
    if(prob == 1)  return(rep.int(size, n))

    if(!identical(cache$k, 0:size) || cache$prob != prob){
      cache$k   <<- 0:size
      cache$cdf <<- pbinom(cache$k, size, prob)
      cache$prob<<- prob
    }
    cache$k[ findInterval(runif(n), cache$cdf) + 1L ]
  }
})

## (2) sum of Bernoulli trials --------------------------------------------
rbinom_sum <- function(n, size, prob){
  if(size == 0L || prob == 0) return(integer(n))
  if(prob == 1)  return(rep.int(size, n))

  matrix(runif(n*size) < prob, n) |> rowSums()
}

## (3) single-uniform “chain” ---------------------------------------------
rbinom_chain <- function(n, size, prob){
  if(size == 0L || prob == 0) return(integer(n))
  if(prob == 1)  return(rep.int(size, n))

  out <- integer(n)
  u   <- runif(n)                       # one U(0,1) per variate
  for(k in seq_len(size)){
    hit      <- u <= prob               # Bernoulli success?
    out[hit] <- out[hit] + 1L
    u[hit]   <-  u[hit]          /  prob          # U/π  | U≤π
    u[!hit]  <- (u[!hit] - prob) / (1 - prob)     # (U-π)/(1-π) | U>π
  }
  out
}

## convenient wrapper ------------------------------------------------------
rbinom_cs <- function(n, size, prob,
                      method = c("inv","sum","chain")){
  switch(match.arg(method),
         inv   = rbinom_inv  (n, size, prob),
         sum   = rbinom_sum  (n, size, prob),
         chain = rbinom_chain(n, size, prob))
}
```

---

### 4.2 Correctness check (one example)

```{r 8}
set.seed(1)
n <- 5e4; m <- 20; p <- 0.3
x <- rbinom_cs(n, m, p, method = "chain")           # pick any method

chisq.test(table(factor(x, levels = 0:m)),
           p = dbinom(0:m, m, p))$p.value
```

The p-value is typically well above 0.05, so we find no evidence
against the Binomial $(20,0.3)$ hypothesis.

---

### 4.3 Timing comparison

```{r 9}
library(microbenchmark)

tim <- microbenchmark(
  inversion = rbinom_inv  (n, m, p),
  bern_sum  = rbinom_sum  (n, m, p),
  chain     = rbinom_chain(n, m, p),
  times = 20L
)
tim
```

| method        | fastest → slowest (median time)      |
| ------------- | ------------------------------------ |
| inversion     | **fastest** (one vectorised `runif`) |
| chain         | second (one `runif`, light loop)     |
| Bernoulli sum | slowest ($n\times m$ uniforms)       |

---

# Exercise 5

 We want to generate pseudo-random numbers from a Beta(3, 1)-distribution which has the density f(x) = 3x^2
 using only runif implemented for random number generation in R.

 ## Write a function implementing the inversion method.


```{r 10}

library(GoFKernel)
library(dplyr)

# Using the fact that a the cdf of the Beta(3, 1) 
# distribution can be approximated by the binomial distribution with parameters n = 3, k = 0,  and p = 1-x.
# The cumulative distribution function:

cdf_binomial_alpha_3_beta_1 <- function(x) {
  ifelse(x < 0, 0, ifelse(x > 1, 1, pbinom(0, 3, 1 - x)))
}

cdf_inverse_cdf_binomial_alpha_3_beta_1 <- inverse(cdf_binomial_alpha_3_beta_1, lower = 0, upper = 1)

# Checking that it works:
cdf_inverse_cdf_binomial_alpha_3_beta_1(0.2)
cdf_binomial_alpha_3_beta_1(cdf_inverse_cdf_binomial_alpha_3_beta_1(0.2))

# Generating random numbers using the inverse method:

gen_inv_beta_3_1 <- function(n) {
sample <- runif(n)
beta_3_1_sample <- numeric(n)
for (i in 1:n) {
beta_3_1_sample[i] <- cdf_inverse_cdf_binomial_alpha_3_beta_1(sample[i])
}
return(beta_3_1_sample)
}

# Measure execution time and save in time_1
time_1 <- system.time({
    gen_inv_beta_3_1(10000)
})

```
## Write a function which implements a transformation method.



```{r 11}
library(lmomco)
# From the class slides, we know that the random variable X = U / (U + V)
# where U ~ Gamma(r, lambda) and V ~ Gamma(s, lambda) is distributed as Beta(r, s)
# so that
n = 1000
sample <- runif(n)
lambda <- 1

cdf_gamma_3_lambda <- function(x) {
    pgamma(x,3,lambda)
}

cdf_inverse_gamma_3 <- function(p, shape = 3, rate = 1, lower = 0, upper = 100000) {
    uniroot(function(x) pgamma(x, shape = shape, rate = rate) - p, lower = lower, upper = upper)$root
}

cdf_inverse_gamma_1 <- function(p, shape = 1, rate = 1, lower = 0, upper = 100000) {
    uniroot(function(x) pgamma(x, shape = shape, rate = rate) - p, lower = lower, upper = upper)$root
}

# Checking that it works: (There is a small error in the last digit)
cdf_inverse_gamma_3(cdf_gamma_3_lambda(20))

# Function to generate samples from the inverse CDF
gen_transf_beta_3_1 <- function(n) {
    sample <- runif(n)
    sample_2 <- runif(n)
    U <- numeric(n)
    V <- numeric(n)
    B <- numeric(n)

    for (i in 1:n) {
        U[i] <- cdf_inverse_gamma_3(sample[i])
        V[i] <- cdf_inverse_gamma_1(sample_2[i])
        B[i] <- U[i] / (U[i] + V[i])
    }
    
    return(B)
}

# Measure execution time and save in time_2
time_2 <- system.time({
    Beta_random <- gen_transf_beta_3_1(10000)
})


```

 ## Write a function which implements rejection sampling using a suitable proposal density.
 
```{r 12}

# Follwing the example in the class, we know that the highest
# value of the density function provided is at 


# Plot the density function of Beta(3,1)
curve(dbeta(x, 3, 1), from = 0, to = 1, lwd = 2, col = "blue",
    ylab = "Density", xlab = "x", main = "Density of Beta(3,1)")

# The density funciotn of the proposalt destribution is such 
# that it is monotonically increasing and has a maximum at 1 with
# a value of 3.

# We establish the minimal possible M equal to 3
# And choose as distribution for Y the uniform distribution in 0,1
M <- 3

rbeta31_rs <- function(n) {
    x <- numeric(n)
    i <- 1
    while (i <= n) {
        y <- runif(1)
        u <- runif(1)
        if (u <= y^2 ) {
            x[i] <- y
            i <- i + 1
        }
    }
    return(x)
}

time_3 <- system.time({
   rbeta31_rs(10000)
})

print(time_1)
print(time_2)
print(time_3)

# The ‘user time’ is the CPU time charged for the execution of user instructions of the calling process. The ‘system time’ is the CPU time charged for execution by the system on behalf of the calling process.

```
 
 
 • Compare the computational performance of these implementations.

The Rejection Sampling Method seems to be the fastest, followed by the Inversion Method and then the Transformation Method.
