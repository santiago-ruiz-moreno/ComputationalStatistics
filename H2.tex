% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Homework 2},
  pdfauthor={Santiago Ruiz, Nikita Karetnikov, Felix Ubl},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{\textbf{Homework 2}}
\author{Santiago Ruiz, Nikita Karetnikov, Felix Ubl}
\date{2025-06-02}

\begin{document}
\maketitle

\subsection{Exercise 1}\label{exercise-1}

In this exercise we need to write a function that implements
Newton-Raphson method. Let us first copy the material from the slides
about this method:

Let \(f : \mathcal{I} \to \mathbb{R}\) be a convex, once continuously
differentiable function with at least one root in \(\mathcal{I}\). Then
the sequence converges to a root.

\[
x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)}
\]

Good! As we understand, to find the root we would need to implement the
cycle in R, where we will use the function and its derivative.

The function is given in the task:

\[
f(x) = x^3 - 4x^2 + 18x - 115.
\] Let us implement it:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{target\_function }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x)\{}
\NormalTok{  y }\OtherTok{=}\NormalTok{ x}\SpecialCharTok{\^{}}\DecValTok{3} \SpecialCharTok{{-}} \DecValTok{4}\SpecialCharTok{*}\NormalTok{x}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{+} \DecValTok{18}\SpecialCharTok{*}\NormalTok{x }\SpecialCharTok{{-}} \DecValTok{115}

  \FunctionTok{return}\NormalTok{(y)    }
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

The derivative of the function is:

\[
f'(x) = 3x^2 - 8x + 18.
\]

Let us implement it:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{derivative }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x)\{}
\NormalTok{  y }\OtherTok{=} \DecValTok{3}\SpecialCharTok{*}\NormalTok{x}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{{-}} \DecValTok{8}\SpecialCharTok{*}\NormalTok{x }\SpecialCharTok{+} \DecValTok{18}

  \FunctionTok{return}\NormalTok{(y)    }
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Now let us implement the cycle.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x\_0 }\OtherTok{=} \DecValTok{10} \CommentTok{\# initial guess}

\NormalTok{x\_n }\OtherTok{=}\NormalTok{ x\_0}
\NormalTok{x\_n\_prev }\OtherTok{=} \DecValTok{0}

\NormalTok{epsilon }\OtherTok{\textless{}{-}}\NormalTok{ .Machine}\SpecialCharTok{$}\NormalTok{double.eps}
\ControlFlowTok{while}\NormalTok{ ( }\FunctionTok{abs}\NormalTok{(x\_n\_prev}\SpecialCharTok{{-}}\NormalTok{x\_n)}\SpecialCharTok{\textgreater{}}\NormalTok{epsilon)\{}
\NormalTok{  x\_n\_prev }\OtherTok{=}\NormalTok{ x\_n}
\NormalTok{  x\_n }\OtherTok{=}\NormalTok{ x\_n\_prev }\SpecialCharTok{{-}}  \FunctionTok{target\_function}\NormalTok{(x\_n\_prev)}\SpecialCharTok{/}\FunctionTok{derivative}\NormalTok{(x\_n\_prev)}
\NormalTok{\}}

\NormalTok{root }\OtherTok{=}\NormalTok{ x\_n}
\FunctionTok{print}\NormalTok{(root)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 5
\end{verbatim}

Let us verify that the computed value is indeed a root of the function
by evaluating \(f(x_n)\) and checking whether it is equal to zero
(within numerical precision):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{target\_function}\NormalTok{(root) }\SpecialCharTok{==} \DecValTok{0}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

Looks very good!

\section{Exercise 2}\label{exercise-2}

In this exercise, we assume lifetimes of light bulbs follow an
exponential distribution with parameter \(\lambda\). Data arise from two
experiments:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Complete data}: failure times \(u_1, \dots, u_n\) recorded
  exactly.
\item
  \textbf{Censored data}: out of \(m\) bulbs observed until time \(t\),
  only the total count \(r\) failures by \(t\) are recorded; individual
  failure times \(v_j\) are missing.
\end{enumerate}

\subsection{(i) Log‑likelihood}\label{i-loglikelihood}

The combined log‑likelihood (up to additive constants) is:

\[
\ell(\lambda; u, r, t, m)
= n\log\lambda - \lambda \sum_{i=1}^n u_i
  + r\log\bigl(1 - e^{-\lambda t}\bigr)
  + (m - r)(-\lambda t).
\]

\subsection{(ii) R function for the
log‑likelihood}\label{ii-r-function-for-the-loglikelihood}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{loglik }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(lambda, u, r, t, m) \{}
  \ControlFlowTok{if}\NormalTok{ (lambda }\SpecialCharTok{\textless{}=} \DecValTok{0}\NormalTok{) }\FunctionTok{return}\NormalTok{(}\SpecialCharTok{{-}}\ConstantTok{Inf}\NormalTok{)}
\NormalTok{  term1 }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(u)}\SpecialCharTok{*}\FunctionTok{log}\NormalTok{(lambda) }\SpecialCharTok{{-}}\NormalTok{ lambda }\SpecialCharTok{*} \FunctionTok{sum}\NormalTok{(u)}
\NormalTok{  term2 }\OtherTok{\textless{}{-}}\NormalTok{ r }\SpecialCharTok{*} \FunctionTok{log}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}} \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{lambda }\SpecialCharTok{*}\NormalTok{ t)) }\SpecialCharTok{+}\NormalTok{ (m }\SpecialCharTok{{-}}\NormalTok{ r) }\SpecialCharTok{*}\NormalTok{ (}\SpecialCharTok{{-}}\NormalTok{lambda }\SpecialCharTok{*}\NormalTok{ t)}
\NormalTok{  term1 }\SpecialCharTok{+}\NormalTok{ term2}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\subsection{\texorpdfstring{(iii) Maximum‐likelihood estimation via
\texttt{optim()}}{(iii) Maximum‐likelihood estimation via optim()}}\label{iii-maximumlikelihood-estimation-via-optim}

We generate artificial data and maximize \(\ell(\lambda)\):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{)}
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{100}\NormalTok{; m }\OtherTok{\textless{}{-}} \DecValTok{20}\NormalTok{; t }\OtherTok{\textless{}{-}} \DecValTok{3}
\NormalTok{u }\OtherTok{\textless{}{-}} \FunctionTok{rexp}\NormalTok{(n, }\DecValTok{2}\NormalTok{)}
\NormalTok{v }\OtherTok{\textless{}{-}} \FunctionTok{rexp}\NormalTok{(m, }\DecValTok{2}\NormalTok{)}
\NormalTok{r }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(v }\SpecialCharTok{\textless{}=}\NormalTok{ t)}

\CommentTok{\# Negative log‑likelihood for minimization:}
\NormalTok{nll }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(lam) }\SpecialCharTok{{-}}\FunctionTok{loglik}\NormalTok{(lam, }\AttributeTok{u =}\NormalTok{ u, }\AttributeTok{r =}\NormalTok{ r, }\AttributeTok{t =}\NormalTok{ t, }\AttributeTok{m =}\NormalTok{ m)}
\NormalTok{opt1 }\OtherTok{\textless{}{-}} \FunctionTok{optim}\NormalTok{(}\AttributeTok{par =} \DecValTok{1}\NormalTok{, }\AttributeTok{fn =}\NormalTok{ nll,}
              \AttributeTok{method =} \StringTok{"Brent"}\NormalTok{, }\AttributeTok{lower =} \FloatTok{0.0001}\NormalTok{, }\AttributeTok{upper =} \DecValTok{10}\NormalTok{)}
\NormalTok{mle\_lambda }\OtherTok{\textless{}{-}}\NormalTok{ opt1}\SpecialCharTok{$}\NormalTok{par}
\FunctionTok{cat}\NormalTok{(}\StringTok{"MLE via optim(): lambda ="}\NormalTok{, mle\_lambda, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## MLE via optim(): lambda = 2.053554
\end{verbatim}

\section{Exercise 3}\label{exercise-3}

We implement the golden‐section search to maximize a univariate
function.

\subsection{(i) Golden‐section search
function}\label{i-goldensection-search-function}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{GoldenSection }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(f, interval, ..., }\AttributeTok{tol =} \FloatTok{1e{-}6}\NormalTok{, }\AttributeTok{iter.max =} \DecValTok{100}\NormalTok{) \{}
\NormalTok{  a }\OtherTok{\textless{}{-}}\NormalTok{ interval[}\DecValTok{1}\NormalTok{]; b }\OtherTok{\textless{}{-}}\NormalTok{ interval[}\DecValTok{2}\NormalTok{]}
\NormalTok{  gr }\OtherTok{\textless{}{-}}\NormalTok{ (}\FunctionTok{sqrt}\NormalTok{(}\DecValTok{5}\NormalTok{) }\SpecialCharTok{+} \DecValTok{1}\NormalTok{) }\SpecialCharTok{/} \DecValTok{2}
\NormalTok{  c }\OtherTok{\textless{}{-}}\NormalTok{ b }\SpecialCharTok{{-}}\NormalTok{ (b }\SpecialCharTok{{-}}\NormalTok{ a) }\SpecialCharTok{/}\NormalTok{ gr}
\NormalTok{  d }\OtherTok{\textless{}{-}}\NormalTok{ a }\SpecialCharTok{+}\NormalTok{ (b }\SpecialCharTok{{-}}\NormalTok{ a) }\SpecialCharTok{/}\NormalTok{ gr}
\NormalTok{  fc }\OtherTok{\textless{}{-}} \FunctionTok{f}\NormalTok{(c, ...); fd }\OtherTok{\textless{}{-}} \FunctionTok{f}\NormalTok{(d, ...)}
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \FunctionTok{seq\_len}\NormalTok{(iter.max)) \{}
    \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{abs}\NormalTok{(b }\SpecialCharTok{{-}}\NormalTok{ a) }\SpecialCharTok{\textless{}}\NormalTok{ tol) }\ControlFlowTok{break}
    \ControlFlowTok{if}\NormalTok{ (fc }\SpecialCharTok{\textgreater{}}\NormalTok{ fd) \{}
\NormalTok{      b }\OtherTok{\textless{}{-}}\NormalTok{ d; d }\OtherTok{\textless{}{-}}\NormalTok{ c; fd }\OtherTok{\textless{}{-}}\NormalTok{ fc}
\NormalTok{      c }\OtherTok{\textless{}{-}}\NormalTok{ b }\SpecialCharTok{{-}}\NormalTok{ (b }\SpecialCharTok{{-}}\NormalTok{ a) }\SpecialCharTok{/}\NormalTok{ gr; fc }\OtherTok{\textless{}{-}} \FunctionTok{f}\NormalTok{(c, ...)}
\NormalTok{    \} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{      a }\OtherTok{\textless{}{-}}\NormalTok{ c; c }\OtherTok{\textless{}{-}}\NormalTok{ d; fc }\OtherTok{\textless{}{-}}\NormalTok{ fd}
\NormalTok{      d }\OtherTok{\textless{}{-}}\NormalTok{ a }\SpecialCharTok{+}\NormalTok{ (b }\SpecialCharTok{{-}}\NormalTok{ a) }\SpecialCharTok{/}\NormalTok{ gr; fd }\OtherTok{\textless{}{-}} \FunctionTok{f}\NormalTok{(d, ...)}
\NormalTok{    \}}
\NormalTok{  \}}
\NormalTok{  x\_opt }\OtherTok{\textless{}{-}}\NormalTok{ (a }\SpecialCharTok{+}\NormalTok{ b) }\SpecialCharTok{/} \DecValTok{2}
  \FunctionTok{list}\NormalTok{(}\AttributeTok{estimate =}\NormalTok{ x\_opt, }\AttributeTok{value =} \FunctionTok{f}\NormalTok{(x\_opt, ...))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\subsection{(ii) Application to the light‑bulb
log‑likelihood}\label{ii-application-to-the-lightbulb-loglikelihood}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Maximize the log‑likelihood from Exercise 2:}
\NormalTok{gs\_res }\OtherTok{\textless{}{-}} \FunctionTok{GoldenSection}\NormalTok{(}
  \ControlFlowTok{function}\NormalTok{(lam) }\FunctionTok{loglik}\NormalTok{(lam, }\AttributeTok{u =}\NormalTok{ u, }\AttributeTok{r =}\NormalTok{ r, }\AttributeTok{t =}\NormalTok{ t, }\AttributeTok{m =}\NormalTok{ m),}
  \AttributeTok{interval =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.001}\NormalTok{, }\DecValTok{10}\NormalTok{), }\AttributeTok{tol =} \FloatTok{1e{-}8}
\NormalTok{)}
\FunctionTok{cat}\NormalTok{(}\StringTok{"GoldenSection: lambda ="}\NormalTok{, gs\_res}\SpecialCharTok{$}\NormalTok{estimate,}
    \StringTok{" value ="}\NormalTok{, gs\_res}\SpecialCharTok{$}\NormalTok{value, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## GoldenSection: lambda = 2.053554  value = -28.34572
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Compare to built{-}in optimize():}
\NormalTok{opt\_res }\OtherTok{\textless{}{-}} \FunctionTok{optimize}\NormalTok{(}
  \ControlFlowTok{function}\NormalTok{(lam) }\FunctionTok{loglik}\NormalTok{(lam, }\AttributeTok{u =}\NormalTok{ u, }\AttributeTok{r =}\NormalTok{ r, }\AttributeTok{t =}\NormalTok{ t, }\AttributeTok{m =}\NormalTok{ m),}
  \AttributeTok{interval =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.001}\NormalTok{, }\DecValTok{10}\NormalTok{), }\AttributeTok{maximum =} \ConstantTok{TRUE}
\NormalTok{)}
\FunctionTok{cat}\NormalTok{(}\StringTok{"optimize():    lambda ="}\NormalTok{, opt\_res}\SpecialCharTok{$}\NormalTok{maximum,}
    \StringTok{" value ="}\NormalTok{, opt\_res}\SpecialCharTok{$}\NormalTok{objective, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## optimize():    lambda = 2.053573  value = -28.34572
\end{verbatim}

Both methods yield essentially the same MLE for \(\lambda\). The
numerical agreement confirms our golden‐section implementation.

\section{Exercise 4}\label{exercise-4}

It is assumed that the lifetime of light bulbs follows an exponential
distribution with parameter \(\lambda\). To estimate \(\lambda\), n
light bulbs were tested until they all failed. Their failure times were
recorded as \(u_{1}, . . . , u_{n}\). In a separate experiment, m bulbs
were tested, but the individual failure times were not recorded. Only
the number of bulbs, r, that had failed at time t was recorded. The
missing data are the failure times of the bulbs in the second
experiment, \(v_{1}, . . . , v_{m}\).

\subsection{i) Determine the complete-data
log-likelihood.}\label{i-determine-the-complete-data-log-likelihood.}

The complete data log-likelihood is defined as follows:

\[
x \sim \text{Exp}(\lambda), \quad f(x) = \lambda e^{-\lambda x}
\] \[
\text{Likelihood}(x) = \prod_{i=1}^{n+m} \lambda e^{-\lambda x_{i}}
\] \[
\text{Log-Likelihood}(x) = \sum_{i=1}^{n+m} \log(\lambda e^{-\lambda x_{i}}) = \sum_{i=1}^{n+m} \log(\lambda) - \lambda x_{i}
\]

\subsection{ii) Determine the conditional
means}\label{ii-determine-the-conditional-means}

By Bayes' rule, we have:

\[
P(x \mid y) = \frac{P(y, x)}{P(y)}
\]

Let \(f\) denote the density function of the exponential distribution.
Then we have:

\[
P(X \leq t) = \frac{f(s)}{P(X \leq t)}
\]

The expression in the denominator is the cumulative distribution
function (CDF) of the exponential distribution, which is given by:

\[
P(X \leq t) = 1 - e^{-\lambda t}
\]

The support of this new density is given by \([0, t]\), and the density
function is:

\[
g(s) = \frac{\lambda e^{-\lambda s}}{1 - e^{-\lambda t}}
\]

Now, define the random variable \(Y = X + Nt\), where
\(N = \max\{n \in \mathbb{N} : nt < X\}\). Then we have:

\[
P(nt < X) = 1 - P(X \leq nt) = (e^{-\lambda t})^n = P(X \leq t)^n
\]

In other words, \(N\) is a random variable that counts the number of
failures before time \(t\). We can define such a probability in terms of
success, meaning in terms of the probability of not having a failure
until time \(t\), \(P(X > t) = 1 - e^{-\lambda t}\). Thus, \(N\) follows
a geometric distribution with parameter \(p = P(X > t)\).

Since \(X\) and \(Y\) are independent, we can write the joint
distribution as:

\[
P(X, Y) = P(X)P(Y)
\]

\[
P(N = n, Y \leq s) = P(nt < X \leq nt + s) = F(nt + s) - F(nt) = e^{-\lambda nt}(1 - e^{-\lambda s})
\]

If we multiply and divide by \(1 - e^{-\lambda t}\), we get the PDF of
the random variable \(N\) times the cumulative distribution function of
the random variable \(Y\):

\[
P(N = n, Y \leq s) = (e^{-\lambda t})^n (1 - e^{-\lambda t}) \frac{1 - e^{-\lambda s}}{1 - e^{-\lambda t}}
\]

Therefore, the CDF of \(Y\) is given by:

\[
G(s) = \frac{1 - e^{-\lambda s}}{1 - e^{-\lambda t}}
\]

Differentiating with respect to \(s\), we get the PDF of \(Y\):

\[
g(s) = \frac{\lambda e^{-\lambda s}}{1 - e^{-\lambda t}}
\]

Thus, \(Y\) is the random variable that counts the average value of the
light bulbs that failed before time \(t\).

\[
E[Y] = E[X] + tE[N] = \frac{1}{\lambda} + \frac{e^{-\lambda t}}{1 - e^{-\lambda t}}
\]

Now, as for \(E[X| X > t] = t + \frac{1}{\lambda}\) because the
exponential distribution is memoryless.

\subsection{iii) Determine the E- and M-step of the EM
algorithm.}\label{iii-determine-the-e--and-m-step-of-the-em-algorithm.}

Let us define a variable \(Z\) that takes the value of
\(Z = I(x \leq t)\) and 0 otherwise. We can compute the expected value
of \(Z\) as follows:

\[E[X | Z = z] =   z\left( \frac{1}{\lambda} + \frac{e^{-\lambda t}}{1 - e^{-\lambda t}} \right) + (1-z)(t + \frac{1}{\lambda}) \]

The EM algorithm consists of two steps: the E-step and the M-step.

The E-step computes the expected value of the complete-data
log-likelihood given the observed data and the current estimate of the
parameters. The M-step maximizes this expected value with respect to the
parameters.

In the E-Step we compute the value of \[ \lambda \] based on its
previous value then we update in the M-step. Namely, in the E-step we
compute the expectation below given the current value of \[ \lambda \]:

\[ E[\text{Log-Likelihood}(x)|Z = z,\lambda_{k}, x_{i}] =  N\log(\lambda_{k}) - \lambda_{k} \sum_{i=1}^{n+m}E[x_{i}| Z = z,\lambda_{k}] \]

In the M step, we use the values provided by remplacing \(v_{i}\) with
its expectation, and maximize to finf the new value of
\[ \lambda_{k+1} \]:

The value \(\lambda_{k+1}\) that maximizes the expected log-likelihood
is given by:

\[ \lambda_{k+1} = \frac{N}{\sum_{i=1}^{n}u_{i} + \sum_{j=1}^{m}E[x_{i}|Z = z,\lambda_{k}]} \]

\subsection{iv) Implement the EM algorithm and apply it to artificial
data generated
with:}\label{iv-implement-the-em-algorithm-and-apply-it-to-artificial-data-generated-with}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{100}\NormalTok{; m }\OtherTok{\textless{}{-}} \DecValTok{20}\NormalTok{; t }\OtherTok{\textless{}{-}} \DecValTok{3}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{)}

\CommentTok{\# True lambda}
\NormalTok{lambda\_true }\OtherTok{\textless{}{-}} \DecValTok{2}

\CommentTok{\# Generate observed and censored data}
\NormalTok{u }\OtherTok{\textless{}{-}} \FunctionTok{rexp}\NormalTok{(n, lambda\_true)         }\CommentTok{\# fully observed failure times}
\NormalTok{v }\OtherTok{\textless{}{-}} \FunctionTok{rexp}\NormalTok{(m, lambda\_true)         }\CommentTok{\# partially observed (we only know how many fail before t)}
\NormalTok{r }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(v }\SpecialCharTok{\textless{}=}\NormalTok{ t)                  }\CommentTok{\# number of censored bulbs that failed before time t}

\CommentTok{\# Initial estimate using only observed data}
\CommentTok{\# lambda\_init \textless{}{-} n / sum(u)}
\NormalTok{lambda\_init }\OtherTok{\textless{}{-}} \DecValTok{35}

\CommentTok{\# Initialize vector for expected values}
\NormalTok{v\_em }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(m)}

\CommentTok{\# Expected value of truncated exponential (0 ≤ X ≤ t)}
\NormalTok{expected\_truncated }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(lambda, t) \{}
  \FunctionTok{return}\NormalTok{((}\DecValTok{1} \SpecialCharTok{/}\NormalTok{ lambda) }\SpecialCharTok{{-}}\NormalTok{ (t }\SpecialCharTok{*} \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{lambda }\SpecialCharTok{*}\NormalTok{ t)) }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}} \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{lambda }\SpecialCharTok{*}\NormalTok{ t)))}
\NormalTok{\}}

\CommentTok{\# Expected value of exponential given X \textgreater{} t (memoryless property)}
\NormalTok{expected\_censored }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(lambda, t) \{}
  \FunctionTok{return}\NormalTok{(t }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{/}\NormalTok{ lambda))}
\NormalTok{\}}

\CommentTok{\# EM Algorithm}
\NormalTok{lambda }\OtherTok{\textless{}{-}}\NormalTok{ lambda\_init}

\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{) \{}
  \CommentTok{\# E{-}step: compute expected values for the m partially observed failure times}
  \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{m) \{}
    \ControlFlowTok{if}\NormalTok{ (j }\SpecialCharTok{\textless{}=}\NormalTok{ r) \{}
\NormalTok{      v\_em[j] }\OtherTok{\textless{}{-}} \FunctionTok{expected\_truncated}\NormalTok{(lambda, t)  }\CommentTok{\# Expected failure time given v\_j ≤ t}
\NormalTok{    \} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{      v\_em[j] }\OtherTok{\textless{}{-}} \FunctionTok{expected\_censored}\NormalTok{(lambda, t)   }\CommentTok{\# Expected failure time given v\_j \textgreater{} t}
\NormalTok{    \}}
\NormalTok{  \}}

  \CommentTok{\# M{-}step: update lambda using complete expected data}
\NormalTok{  lambda }\OtherTok{\textless{}{-}}\NormalTok{ (n }\SpecialCharTok{+}\NormalTok{ m) }\SpecialCharTok{/}\NormalTok{ (}\FunctionTok{sum}\NormalTok{(u) }\SpecialCharTok{+} \FunctionTok{sum}\NormalTok{(v\_em))}

  \FunctionTok{print}\NormalTok{(lambda)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2.429424
## [1] 2.104733
## [1] 2.061269
## [1] 2.054733
## [1] 2.053734
## [1] 2.053581
## [1] 2.053558
## [1] 2.053554
## [1] 2.053554
## [1] 2.053554
\end{verbatim}

\section{Exercise 5}\label{exercise-5}

\subsection{\texorpdfstring{i) Specify the log-likelihood for a single
observation \(y_{i}\). Assume for \(y_{i}\) that the first \(d_{1}\)
variables are observed and that the next \(d_{2}\) variables are
missing,
i.e.,}{i) Specify the log-likelihood for a single observation y\_\{i\}. Assume for y\_\{i\} that the first d\_\{1\} variables are observed and that the next d\_\{2\} variables are missing, i.e.,}}\label{i-specify-the-log-likelihood-for-a-single-observation-y_i.-assume-for-y_i-that-the-first-d_1-variables-are-observed-and-that-the-next-d_2-variables-are-missing-i.e.}

\[
\ell(\mathbf{y}_i; \boldsymbol{\mu}, \boldsymbol{\Sigma}) = 
-\frac{1}{2} \left[ d \log(2\pi) + \log|\boldsymbol{\Sigma}| + (\mathbf{y}_i - \boldsymbol{\mu})^\top \boldsymbol{\Sigma}^{-1} (\mathbf{y}_i - \boldsymbol{\mu}) \right]
\]

\subsection{ii) Implement a function loglik which given the data and the
parameter values μ and Σ returns the log-likelihood
values.}\label{ii-implement-a-function-loglik-which-given-the-data-and-the-parameter-values-ux3bc-and-ux3c3-returns-the-log-likelihood-values.}

\subsection{iii) The parameters need to be vectorized for the general
purpose optimizer. Suggest a suitable vectorizing
scheme.}\label{iii-the-parameters-need-to-be-vectorized-for-the-general-purpose-optimizer.-suggest-a-suitable-vectorizing-scheme.}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(mvtnorm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: package 'mvtnorm' was built under R version 4.2.3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{loglik }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(y, mu, sigma) \{}
\NormalTok{  d }\OtherTok{\textless{}{-}} \FunctionTok{ncol}\NormalTok{(y)}
\NormalTok{  n }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(y)}
\NormalTok{  log\_likelihood }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(n)}

  \CommentTok{\# Check for valid sigma}
  \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{any}\NormalTok{(}\FunctionTok{is.na}\NormalTok{(sigma)) }\SpecialCharTok{||} \FunctionTok{det}\NormalTok{(sigma) }\SpecialCharTok{\textless{}=} \DecValTok{0} \SpecialCharTok{||} \FunctionTok{any}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.finite}\NormalTok{(sigma))) \{}
    \FunctionTok{return}\NormalTok{(}\ConstantTok{NA}\NormalTok{)}
\NormalTok{  \}}

  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{n) \{}
\NormalTok{    y\_i }\OtherTok{\textless{}{-}}\NormalTok{ y[i, ]}
\NormalTok{    diff }\OtherTok{\textless{}{-}}\NormalTok{ y\_i }\SpecialCharTok{{-}}\NormalTok{ mu}
\NormalTok{    ll }\OtherTok{\textless{}{-}} \FunctionTok{tryCatch}\NormalTok{(\{}
      \SpecialCharTok{{-}}\FloatTok{0.5} \SpecialCharTok{*}\NormalTok{ (d }\SpecialCharTok{*} \FunctionTok{log}\NormalTok{(}\DecValTok{2} \SpecialCharTok{*}\NormalTok{ pi) }\SpecialCharTok{+} \FunctionTok{log}\NormalTok{(}\FunctionTok{det}\NormalTok{(sigma)) }\SpecialCharTok{+}
              \FunctionTok{t}\NormalTok{(diff) }\SpecialCharTok{\%*\%} \FunctionTok{solve}\NormalTok{(sigma) }\SpecialCharTok{\%*\%}\NormalTok{ diff)}
\NormalTok{    \}, }\AttributeTok{error =} \ControlFlowTok{function}\NormalTok{(e) }\ConstantTok{NA}\NormalTok{)}
\NormalTok{    log\_likelihood[i] }\OtherTok{\textless{}{-}}\NormalTok{ ll}
\NormalTok{  \}}
  \FunctionTok{return}\NormalTok{(}\FunctionTok{sum}\NormalTok{(log\_likelihood))}
\NormalTok{\}}

\CommentTok{\# Create a wrapper to unpack parameters}
\NormalTok{neg\_loglik\_wrapper }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(params, y, d) \{}
\NormalTok{  mu }\OtherTok{\textless{}{-}}\NormalTok{ params[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{d]}
\NormalTok{  L\_vals }\OtherTok{\textless{}{-}}\NormalTok{ params[(d }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(params)]}
  
  \CommentTok{\# Reconstruct lower triangle matrix}
\NormalTok{  L }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\DecValTok{0}\NormalTok{, d, d)}
\NormalTok{  L[}\FunctionTok{lower.tri}\NormalTok{(L, }\AttributeTok{diag =} \ConstantTok{TRUE}\NormalTok{)] }\OtherTok{\textless{}{-}}\NormalTok{ L\_vals}
  
  \CommentTok{\# sigma = L \%*\% t(L), to ensure positive{-}definite}
\NormalTok{  sigma }\OtherTok{\textless{}{-}}\NormalTok{ L }\SpecialCharTok{\%*\%} \FunctionTok{t}\NormalTok{(L) }\SpecialCharTok{+} \FunctionTok{diag}\NormalTok{(}\FloatTok{1e{-}6}\NormalTok{, d)}
  
  \CommentTok{\# Return negative log{-}likelihood (since optim minimizes)}
  \FunctionTok{return}\NormalTok{(}\SpecialCharTok{{-}}\FunctionTok{loglik}\NormalTok{(y, mu, sigma))}
\NormalTok{\}}

\CommentTok{\# Generate sample data}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{)}
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{100}
\NormalTok{d }\OtherTok{\textless{}{-}} \DecValTok{3}
\NormalTok{true\_mu }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\NormalTok{true\_sigma }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.3}\NormalTok{,}
                       \FloatTok{0.5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.2}\NormalTok{,}
                       \FloatTok{0.3}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{nrow =}\NormalTok{ d)}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{rmvnorm}\NormalTok{(n, }\AttributeTok{mean =}\NormalTok{ true\_mu, }\AttributeTok{sigma =}\NormalTok{ true\_sigma)}

\CommentTok{\# Initial guesses}
\NormalTok{init\_mu }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\DecValTok{3}\NormalTok{, d)}
\NormalTok{init\_L }\OtherTok{\textless{}{-}} \FunctionTok{diag}\NormalTok{(}\DecValTok{1}\NormalTok{, d)}
\NormalTok{init\_L\_vals }\OtherTok{\textless{}{-}}\NormalTok{ init\_L[}\FunctionTok{lower.tri}\NormalTok{(init\_L, }\AttributeTok{diag =} \ConstantTok{TRUE}\NormalTok{)]}
\NormalTok{init\_params }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(init\_mu, init\_L\_vals)}

\CommentTok{\# Optimize}
\NormalTok{result }\OtherTok{\textless{}{-}} \FunctionTok{optim}\NormalTok{(init\_params, neg\_loglik\_wrapper, }\AttributeTok{y =}\NormalTok{ y, }\AttributeTok{d =}\NormalTok{ d, }\AttributeTok{method =} \StringTok{"BFGS"}\NormalTok{,}
                \AttributeTok{control =} \FunctionTok{list}\NormalTok{(}\AttributeTok{maxit =} \DecValTok{1000}\NormalTok{, }\AttributeTok{reltol =} \FloatTok{1e{-}8}\NormalTok{))}

\CommentTok{\# Extract optimized mu and sigma}
\NormalTok{opt\_mu }\OtherTok{\textless{}{-}}\NormalTok{ result}\SpecialCharTok{$}\NormalTok{par[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{d]}
\NormalTok{opt\_l\_vals }\OtherTok{\textless{}{-}}\NormalTok{ result}\SpecialCharTok{$}\NormalTok{par[(d }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(result}\SpecialCharTok{$}\NormalTok{par)]}
\NormalTok{lower\_triangle\_matrix }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\DecValTok{0}\NormalTok{, d, d)}
\NormalTok{lower\_triangle\_matrix[}\FunctionTok{lower.tri}\NormalTok{(lower\_triangle\_matrix, }\AttributeTok{diag =} \ConstantTok{TRUE}\NormalTok{)] }\OtherTok{\textless{}{-}}\NormalTok{ opt\_l\_vals}
\NormalTok{opt\_sigma }\OtherTok{\textless{}{-}}\NormalTok{ lower\_triangle\_matrix }\SpecialCharTok{\%*\%} \FunctionTok{t}\NormalTok{(lower\_triangle\_matrix)}

\CommentTok{\# Show result}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Optimized mu:}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Optimized mu:
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(opt\_mu)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -0.02946748  0.09323005 -0.01456996
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{"Optimized sigma:}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Optimized sigma:
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(opt\_sigma)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           [,1]      [,2]      [,3]
## [1,] 0.8994061 0.6005967 0.2878640
## [2,] 0.6005967 1.0564672 0.4036455
## [3,] 0.2878640 0.4036455 1.1987436
\end{verbatim}

\subsection{iv) Use optim to maximize the log-likelihood on an
artificial data set of size 200 drawn from a 3-dimensional multivariate
normal distribution with mean zero and variance-covariance
matrix}\label{iv-use-optim-to-maximize-the-log-likelihood-on-an-artificial-data-set-of-size-200-drawn-from-a-3-dimensional-multivariate-normal-distribution-with-mean-zero-and-variance-covariance-matrix}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(mvtnorm)}

\CommentTok{\# Generate sample data with missing values}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{)}
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{200}
\NormalTok{d }\OtherTok{\textless{}{-}} \DecValTok{3}
\NormalTok{true\_mu }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\NormalTok{true\_sigma }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.5}\NormalTok{,}
                       \FloatTok{0.5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.5}\NormalTok{,}
                       \FloatTok{0.5}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{nrow =}\NormalTok{ d)}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{rmvnorm}\NormalTok{(n, }\AttributeTok{mean =}\NormalTok{ true\_mu, }\AttributeTok{sigma =}\NormalTok{ true\_sigma)}

\CommentTok{\# Assign 30\% missing values randomly}
\NormalTok{missing\_indices }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{runif}\NormalTok{(n }\SpecialCharTok{*}\NormalTok{ d) }\SpecialCharTok{\textless{}} \FloatTok{0.3}\NormalTok{, }\AttributeTok{nrow =}\NormalTok{ n, }\AttributeTok{ncol =}\NormalTok{ d)}
\NormalTok{y[missing\_indices] }\OtherTok{\textless{}{-}} \ConstantTok{NA}

\CommentTok{\# Log{-}likelihood function}
\NormalTok{loglik }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(y, mu, sigma) \{}
\NormalTok{  d }\OtherTok{\textless{}{-}} \FunctionTok{ncol}\NormalTok{(y)}
\NormalTok{  n }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(y)}
\NormalTok{  log\_likelihood }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(n)}

  \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{any}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.finite}\NormalTok{(mu)) }\SpecialCharTok{||} \FunctionTok{any}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.finite}\NormalTok{(sigma)) }\SpecialCharTok{||} \FunctionTok{det}\NormalTok{(sigma) }\SpecialCharTok{\textless{}=} \DecValTok{0}\NormalTok{) \{}
    \FunctionTok{return}\NormalTok{(}\ConstantTok{NA}\NormalTok{)}
\NormalTok{  \}}

  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{n) \{}
\NormalTok{    y\_i }\OtherTok{\textless{}{-}}\NormalTok{ y[i, ]}
    \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{any}\NormalTok{(}\FunctionTok{is.na}\NormalTok{(y\_i))) }\ControlFlowTok{next}
\NormalTok{    diff }\OtherTok{\textless{}{-}}\NormalTok{ y\_i }\SpecialCharTok{{-}}\NormalTok{ mu}
\NormalTok{    ll }\OtherTok{\textless{}{-}} \FunctionTok{tryCatch}\NormalTok{(\{}
      \SpecialCharTok{{-}}\FloatTok{0.5} \SpecialCharTok{*}\NormalTok{ (d }\SpecialCharTok{*} \FunctionTok{log}\NormalTok{(}\DecValTok{2} \SpecialCharTok{*}\NormalTok{ pi) }\SpecialCharTok{+} \FunctionTok{log}\NormalTok{(}\FunctionTok{det}\NormalTok{(sigma)) }\SpecialCharTok{+}
                \FunctionTok{t}\NormalTok{(diff) }\SpecialCharTok{\%*\%} \FunctionTok{solve}\NormalTok{(sigma) }\SpecialCharTok{\%*\%}\NormalTok{ diff)}
\NormalTok{    \}, }\AttributeTok{error =} \ControlFlowTok{function}\NormalTok{(e) }\ConstantTok{NA}\NormalTok{)}
\NormalTok{    log\_likelihood[i] }\OtherTok{\textless{}{-}}\NormalTok{ ll}
\NormalTok{  \}}
  \FunctionTok{return}\NormalTok{(}\FunctionTok{sum}\NormalTok{(log\_likelihood, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{))}
\NormalTok{\}}

\CommentTok{\# Imputation function}
\NormalTok{impute\_missing }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(y, mu, sigma) \{}
\NormalTok{  y\_imputed }\OtherTok{\textless{}{-}}\NormalTok{ y}
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{nrow}\NormalTok{(y)) \{}
\NormalTok{    obs\_idx }\OtherTok{\textless{}{-}} \SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(y[i, ])}
\NormalTok{    miss\_idx }\OtherTok{\textless{}{-}} \FunctionTok{is.na}\NormalTok{(y[i, ])}
    \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{any}\NormalTok{(miss\_idx) }\SpecialCharTok{\&\&} \FunctionTok{any}\NormalTok{(obs\_idx)) \{}
\NormalTok{      mu\_obs }\OtherTok{\textless{}{-}}\NormalTok{ mu[obs\_idx]}
\NormalTok{      mu\_miss }\OtherTok{\textless{}{-}}\NormalTok{ mu[miss\_idx]}
\NormalTok{      sigma\_oo }\OtherTok{\textless{}{-}}\NormalTok{ sigma[obs\_idx, obs\_idx]}
\NormalTok{      sigma\_mo }\OtherTok{\textless{}{-}}\NormalTok{ sigma[miss\_idx, obs\_idx]}
\NormalTok{      y\_obs }\OtherTok{\textless{}{-}}\NormalTok{ y[i, obs\_idx]}

\NormalTok{      y\_miss\_hat }\OtherTok{\textless{}{-}} \FunctionTok{tryCatch}\NormalTok{(\{}
\NormalTok{        mu\_miss }\SpecialCharTok{+}\NormalTok{ sigma\_mo }\SpecialCharTok{\%*\%} \FunctionTok{solve}\NormalTok{(sigma\_oo) }\SpecialCharTok{\%*\%}\NormalTok{ (y\_obs }\SpecialCharTok{{-}}\NormalTok{ mu\_obs)}
\NormalTok{      \}, }\AttributeTok{error =} \ControlFlowTok{function}\NormalTok{(e) }\FunctionTok{rep}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\FunctionTok{sum}\NormalTok{(miss\_idx)))}

\NormalTok{      y\_imputed[i, miss\_idx] }\OtherTok{\textless{}{-}}\NormalTok{ y\_miss\_hat}
\NormalTok{    \}}
\NormalTok{  \}}
  \FunctionTok{return}\NormalTok{(y\_imputed)}
\NormalTok{\}}

\CommentTok{\# Wrapper for optimization}
\NormalTok{neg\_loglik\_wrapper }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(params, y, d) \{}
\NormalTok{  mu }\OtherTok{\textless{}{-}}\NormalTok{ params[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{d]}
\NormalTok{  L\_vals }\OtherTok{\textless{}{-}}\NormalTok{ params[(d }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(params)]}
\NormalTok{  L }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\DecValTok{0}\NormalTok{, d, d)}
\NormalTok{  L[}\FunctionTok{lower.tri}\NormalTok{(L, }\AttributeTok{diag =} \ConstantTok{TRUE}\NormalTok{)] }\OtherTok{\textless{}{-}}\NormalTok{ L\_vals}
\NormalTok{  sigma }\OtherTok{\textless{}{-}}\NormalTok{ L }\SpecialCharTok{\%*\%} \FunctionTok{t}\NormalTok{(L) }\SpecialCharTok{+} \FunctionTok{diag}\NormalTok{(}\FloatTok{1e{-}6}\NormalTok{, d)  }\CommentTok{\# regularization for stability}
  \FunctionTok{return}\NormalTok{(}\SpecialCharTok{{-}}\FunctionTok{loglik}\NormalTok{(y, mu, sigma))}
\NormalTok{\}}

\CommentTok{\# Initialization}
\NormalTok{init\_mu }\OtherTok{\textless{}{-}} \FunctionTok{colMeans}\NormalTok{(y, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{init\_L }\OtherTok{\textless{}{-}} \FunctionTok{diag}\NormalTok{(}\DecValTok{1}\NormalTok{, d)}
\NormalTok{init\_L\_vals }\OtherTok{\textless{}{-}}\NormalTok{ init\_L[}\FunctionTok{lower.tri}\NormalTok{(init\_L, }\AttributeTok{diag =} \ConstantTok{TRUE}\NormalTok{)]}
\NormalTok{init\_params }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(init\_mu, init\_L\_vals)}

\CommentTok{\# Initial values for opt\_mu and opt\_sigma}
\NormalTok{opt\_mu }\OtherTok{\textless{}{-}}\NormalTok{ init\_mu}
\NormalTok{opt\_sigma }\OtherTok{\textless{}{-}}\NormalTok{ init\_L }\SpecialCharTok{\%*\%} \FunctionTok{t}\NormalTok{(init\_L)}

\CommentTok{\# EM Algorithm}
\ControlFlowTok{for}\NormalTok{ (iter }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{) \{}
  \FunctionTok{cat}\NormalTok{(}\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{{-}{-}{-} Iteration"}\NormalTok{, iter, }\StringTok{"{-}{-}{-}}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}

  \CommentTok{\# E{-}step}
\NormalTok{  y\_imputed }\OtherTok{\textless{}{-}} \FunctionTok{impute\_missing}\NormalTok{(y, opt\_mu, opt\_sigma)}

  \CommentTok{\# M{-}step}
\NormalTok{  result }\OtherTok{\textless{}{-}} \FunctionTok{optim}\NormalTok{(init\_params, neg\_loglik\_wrapper, }\AttributeTok{y =}\NormalTok{ y\_imputed, }\AttributeTok{d =}\NormalTok{ d, }\AttributeTok{method =} \StringTok{"BFGS"}\NormalTok{,}
                  \AttributeTok{control =} \FunctionTok{list}\NormalTok{(}\AttributeTok{maxit =} \DecValTok{1000}\NormalTok{, }\AttributeTok{reltol =} \FloatTok{1e{-}8}\NormalTok{))}

  \CommentTok{\# Update parameters}
\NormalTok{  opt\_mu }\OtherTok{\textless{}{-}}\NormalTok{ result}\SpecialCharTok{$}\NormalTok{par[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{d]}
\NormalTok{  opt\_L\_vals }\OtherTok{\textless{}{-}}\NormalTok{ result}\SpecialCharTok{$}\NormalTok{par[(d }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(result}\SpecialCharTok{$}\NormalTok{par)]}
\NormalTok{  L }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\DecValTok{0}\NormalTok{, d, d)}
\NormalTok{  L[}\FunctionTok{lower.tri}\NormalTok{(L, }\AttributeTok{diag =} \ConstantTok{TRUE}\NormalTok{)] }\OtherTok{\textless{}{-}}\NormalTok{ opt\_L\_vals}
\NormalTok{  opt\_sigma }\OtherTok{\textless{}{-}}\NormalTok{ L }\SpecialCharTok{\%*\%} \FunctionTok{t}\NormalTok{(L)}

\NormalTok{  init\_params }\OtherTok{\textless{}{-}}\NormalTok{ result}\SpecialCharTok{$}\NormalTok{par}

  \CommentTok{\# Output}
  \FunctionTok{cat}\NormalTok{(}\StringTok{"mu:}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{); }\FunctionTok{print}\NormalTok{(opt\_mu)}
  \FunctionTok{cat}\NormalTok{(}\StringTok{"sigma:}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{); }\FunctionTok{print}\NormalTok{(opt\_sigma)}
  \FunctionTok{cat}\NormalTok{(}\StringTok{"log{-}likelihood:"}\NormalTok{, }\FunctionTok{loglik}\NormalTok{(y\_imputed, opt\_mu, opt\_sigma), }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## --- Iteration 1 ---
## mu:
## [1] -0.008317493 -0.022773774 -0.093718452
## sigma:
##           [,1]      [,2]      [,3]
## [1,] 0.6500835 0.2446865 0.1792482
## [2,] 0.2446865 0.7745268 0.2953061
## [3,] 0.1792482 0.2953061 0.7509705
## log-likelihood: -708.6999 
## 
## --- Iteration 2 ---
## mu:
## [1]  0.003065366 -0.006070936 -0.083917269
## sigma:
##           [,1]      [,2]      [,3]
## [1,] 0.6928270 0.4619437 0.3253880
## [2,] 0.4619437 0.8402189 0.5047157
## [3,] 0.3253880 0.5047157 0.7947876
## log-likelihood: -665.9273 
## 
## --- Iteration 3 ---
## mu:
## [1]  0.007791569  0.006893808 -0.072282276
## sigma:
##           [,1]      [,2]      [,3]
## [1,] 0.7583619 0.5946404 0.3922682
## [2,] 0.5946404 0.9241251 0.6075103
## [3,] 0.3922682 0.6075103 0.8457565
## log-likelihood: -651.6274 
## 
## --- Iteration 4 ---
## mu:
## [1]  0.005708927  0.013447549 -0.064367021
## sigma:
##           [,1]      [,2]      [,3]
## [1,] 0.7958350 0.6525081 0.3994355
## [2,] 0.6525081 0.9635697 0.6378150
## [3,] 0.3994355 0.6378150 0.8634479
## log-likelihood: -648.0823 
## 
## --- Iteration 5 ---
## mu:
## [1]  0.002353799  0.016942668 -0.059376083
## sigma:
##           [,1]      [,2]      [,3]
## [1,] 0.8127925 0.6758842 0.3888605
## [2,] 0.6758842 0.9806473 0.6456530
## [3,] 0.3888605 0.6456530 0.8681330
## log-likelihood: -646.8705
\end{verbatim}

\end{document}
