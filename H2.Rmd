---
title: "**Homework 2**"
author: "Santiago Ruiz, Nikita Karetnikov, Felix Ubl "
output:
  pdf_document:
    number_sections: true
date: "`r Sys.Date()`"
---



## Exercise 1

In this exercise we need to write a function that implements Newton-Raphson method.
Let us first  copy the material from the slides about this method:

Let \( f : \mathcal{I} \to \mathbb{R} \) be a convex, once continuously differentiable function with at least one root in \( \mathcal{I} \). Then the sequence converges to a root.


\[
x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)}
\]


Good! As we understand, to find the root we would need to implement the cycle in R,
where we will use the function and its derivative.

The function is given in the task:

\[
f(x) = x^3 - 4x^2 + 18x - 115.
\]
Let us implement it:
```{r}

target_function <- function(x){
  y = x^3 - 4*x^2 + 18*x - 115

  return(y)    
}

```

The derivative of the function is:

\[
f'(x) = 3x^2 - 8x + 18.
\]

Let us implement it:

```{r}

derivative <- function(x){
  y = 3*x^2 - 8*x + 18

  return(y)    
}
```


Now let us implement the cycle.


```{r}
x_0 = 10 # initial guess

x_n = x_0
x_n_prev = 0

epsilon <- .Machine$double.eps
while ( abs(x_n_prev-x_n)>epsilon){
  x_n_prev = x_n
  x_n = x_n_prev -  target_function(x_n_prev)/derivative(x_n_prev)
}

root = x_n
print(root)
```
Let us verify that the computed value is indeed a root of the function
by evaluating \( f(x_n) \) and checking whether
it is equal to zero (within numerical precision):


```{r}
target_function(root) == 0
```
Looks very good!


# Exercise 2

In this exercise, we assume lifetimes of light bulbs follow an exponential distribution with parameter \(\lambda\).  Data arise from two experiments:

1. **Complete data**: failure times \(u_1, \dots, u_n\) recorded exactly.
2. **Censored data**: out of \(m\) bulbs observed until time \(t\), only the total count \(r\) failures by \(t\) are recorded; individual failure times \(v_j\) are missing.

## (i) Log‑likelihood

The combined log‑likelihood (up to additive constants) is:

\[
\ell(\lambda; u, r, t, m)
= n\log\lambda - \lambda \sum_{i=1}^n u_i
  + r\log\bigl(1 - e^{-\lambda t}\bigr)
  + (m - r)(-\lambda t).
\]

## (ii) R function for the log‑likelihood

```{r}
loglik <- function(lambda, u, r, t, m) {
  if (lambda <= 0) return(-Inf)
  term1 <- length(u)*log(lambda) - lambda * sum(u)
  term2 <- r * log(1 - exp(-lambda * t)) + (m - r) * (-lambda * t)
  term1 + term2
}
```

## (iii) Maximum‐likelihood estimation via `optim()`

We generate artificial data and maximize \(\ell(\lambda)\):

```{r}
set.seed(1234)
n <- 100; m <- 20; t <- 3
u <- rexp(n, 2)
v <- rexp(m, 2)
r <- sum(v <= t)

# Negative log‑likelihood for minimization:
nll <- function(lam) -loglik(lam, u = u, r = r, t = t, m = m)
opt1 <- optim(par = 1, fn = nll,
              method = "Brent", lower = 0.0001, upper = 10)
mle_lambda <- opt1$par
cat("MLE via optim(): lambda =", mle_lambda, "\n")
```

# Exercise 3

We implement the golden‐section search to maximize a univariate function.

## (i) Golden‐section search function

```{r}
GoldenSection <- function(f, interval, ..., tol = 1e-6, iter.max = 100) {
  a <- interval[1]; b <- interval[2]
  gr <- (sqrt(5) + 1) / 2
  c <- b - (b - a) / gr
  d <- a + (b - a) / gr
  fc <- f(c, ...); fd <- f(d, ...)
  for (i in seq_len(iter.max)) {
    if (abs(b - a) < tol) break
    if (fc > fd) {
      b <- d; d <- c; fd <- fc
      c <- b - (b - a) / gr; fc <- f(c, ...)
    } else {
      a <- c; c <- d; fc <- fd
      d <- a + (b - a) / gr; fd <- f(d, ...)
    }
  }
  x_opt <- (a + b) / 2
  list(estimate = x_opt, value = f(x_opt, ...))
}
```

## (ii) Application to the light‑bulb log‑likelihood

```{r}
# Maximize the log‑likelihood from Exercise 2:
gs_res <- GoldenSection(
  function(lam) loglik(lam, u = u, r = r, t = t, m = m),
  interval = c(0.001, 10), tol = 1e-8
)
cat("GoldenSection: lambda =", gs_res$estimate,
    " value =", gs_res$value, "\n")

# Compare to built-in optimize():
opt_res <- optimize(
  function(lam) loglik(lam, u = u, r = r, t = t, m = m),
  interval = c(0.001, 10), maximum = TRUE
)
cat("optimize():    lambda =", opt_res$maximum,
    " value =", opt_res$objective, "\n")
```

Both methods yield essentially the same MLE for \(\lambda\).  The numerical agreement confirms our golden‐section implementation.




# Exercise 5

```{r}


```




