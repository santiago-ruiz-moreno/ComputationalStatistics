---
title: "**Homework 1**"
author: "Santiago Ruiz, Nikita Karetnikov, Felix Ubl "
output:
  pdf_document:
    number_sections: true
date: "`r Sys.Date()`"
---




# Exercise 4

It is assumed that the lifetime of light bulbs follows an exponential distribution with parameter $\lambda$. To estimate
$\lambda$, n light bulbs were tested until they all failed. Their failure times were recorded as $u_{1}, . . . , u_{n}$. In a separate
experiment, m bulbs were tested, but the individual failure times were not recorded. Only the number of bulbs, r,
that had failed at time t was recorded. The missing data are the failure times of the bulbs in the second experiment,
$v_{1}, . . . , v_{m}$.

## Determine the complete-data log-likelihood.

The complete data log-likelihood is defined as follows:

\[
x \sim \text{Exp}(\lambda), \quad f(x) = \lambda e^{-\lambda x}
\]
\[
\text{Likelihood}(x) = \prod_{i=1}^{n+m} \lambda e^{-\lambda x_{i}}
\]
\[
\text{Log-Likelihood}(x) = \sum_{i=1}^{n+m} \log(\lambda e^{-\lambda x_{i}}) = \sum_{i=1}^{n+m} \log(\lambda) - \lambda x_{i}
\]

## Determine the conditional means
By Bayes' rule, we have:

\[
P(x \mid y) = \frac{P(y, x)}{P(y)}
\]

Let \( f \) denote the density function of the exponential distribution. Then we have:

\[
P(X \leq t) = \frac{f(s)}{P(X \leq t)}
\]

The expression in the denominator is the cumulative distribution function (CDF) of the exponential distribution, which is given by:

\[
P(X \leq t) = 1 - e^{-\lambda t}
\]

The support of this new density is given by \([0, t]\), and the density function is:

\[
g(s) = \frac{\lambda e^{-\lambda s}}{1 - e^{-\lambda t}}
\]

Now, define the random variable \( Y = X + Nt \), where \( N = \max\{n \in \mathbb{N} : nt < X\} \). Then we have:

\[
P(nt < X) = 1 - P(X \leq nt) = (e^{-\lambda t})^n = P(X \leq t)^n
\]

In other words, \( N \) is a random variable that counts the number of failures before time \( t \). We can define such a probability in terms of success, meaning in terms of the probability of not having a failure until time \( t \), \( P(X > t) = 1 - e^{-\lambda t} \). Thus, \( N \) follows a geometric distribution with parameter \( p = P(X > t) \).

Since \( X \) and \( Y \) are independent, we can write the joint distribution as:

\[
P(X, Y) = P(X)P(Y)
\]

\[
P(N = n, Y \leq s) = P(nt < X \leq nt + s) = F(nt + s) - F(nt) = e^{-\lambda nt}(1 - e^{-\lambda s})
\]

If we multiply and divide by \( 1 - e^{-\lambda t} \), we get the PDF of the random variable \( N \) times the cumulative distribution function of the random variable \( Y \):

\[
P(N = n, Y \leq s) = (e^{-\lambda t})^n (1 - e^{-\lambda t}) \frac{1 - e^{-\lambda s}}{1 - e^{-\lambda t}}
\]

Therefore, the CDF of \( Y \) is given by:

\[
G(s) = \frac{1 - e^{-\lambda s}}{1 - e^{-\lambda t}}
\]

Differentiating with respect to \( s \), we get the PDF of \( Y \):

\[
g(s) = \frac{\lambda e^{-\lambda s}}{1 - e^{-\lambda t}}
\]

Thus, \( Y \) is the random variable that counts the average value of the light bulbs that failed before time \( t \).

\[
E[Y] = E[X] + tE[N] = \frac{1}{\lambda} + \frac{e^{-\lambda t}}{1 - e^{-\lambda t}}
\]


Now, as for $E[X| X > t] = t + \frac{1}{\lambda}$ because the exponential distribution is memoryless. 


## Determine the E- and M-step of the EM algorithm.

Let us define a variable $Z$ that takes
the value of $Z = I(x \leq 1)$ and 0 otherwise. We can compute the expected value of $Z$ as follows:

$$E[X | Z = z] =   z\left( \frac{1}{\lambda} + \frac{e^{-\lambda t}}{1 - e^{-\lambda t}} \right) + (1-z)(t + \frac{1}{\lambda}) $$


The EM algorithm consists of two steps: the E-step and the M-step.

The E-step computes the expected value of the complete-data log-likelihood given the observed data and the current estimate of the parameters. The M-step maximizes this expected value with respect to the parameters.

In the E-Step we compute the value of $$ \lambda $$ based on its previous value then we update in the  M-step.
$$ E[\text{Log-Likelihood}(x)|Y = y,\lambda_{k}] =  N\log(\lambda_{k}) - \lambda_{k} \sum_{i=1}^{n+m}E[x_{i}| Y = y,\lambda_{k}] $$

M step:

The value $\lambda_{k+1}$ that maximizes the expected log-likelihood is given by:

$$ \lambda_{k+1} = \frac{N}{\sum_{i=1}^{n}u_{i} + \sum_{j=1}^{m}E[x_{i}|Z = z,\lambda_{k}]} $$

This can be better writen as :

```{r,  echo=TRUE}

n <- 100; m <- 20; t <- 3
set.seed(1234)

# True lambda
lambda_true <- 2

# Generate observed and censored data
u <- rexp(n, lambda_true)         # fully observed failure times
v <- rexp(m, lambda_true)         # partially observed (we only know how many fail before t)
r <- sum(v <= t)                  # number of censored bulbs that failed before time t

# Initial estimate using only observed data
# lambda_init <- n / sum(u)
lambda_init <- 35

# Initialize vector for expected values
v_em <- numeric(m)

# Expected value of truncated exponential (0 ≤ X ≤ t)
expected_truncated <- function(lambda, t) {
  return((1 / lambda) - (t * exp(-lambda * t)) / (1 - exp(-lambda * t)))
}

# Expected value of exponential given X > t (memoryless property)
expected_censored <- function(lambda, t) {
  return(t + (1 / lambda))
}

# EM Algorithm
lambda <- lambda_init

for (i in 1:10) {
  # E-step: compute expected values for the m partially observed failure times
  for (j in 1:m) {
    if (j <= r) {
      v_em[j] <- expected_truncated(lambda, t)  # Expected failure time given v_j ≤ t
    } else {
      v_em[j] <- expected_censored(lambda, t)   # Expected failure time given v_j > t
    }
  }

  # M-step: update lambda using complete expected data
  lambda <- (n + m) / (sum(u) + sum(v_em))

  print(lambda)
}
```

# Exercise 5

A multivariate numeric data set with missing values is given. We assume that the data come from a multivariate
normal distribution and we want to estimate the parameters using maximum-likelihood estimation with a general
purpose optimizer.

## Specify the log-likelihood for a single observation $y_{i}$. Assume for $y_{i}$ that the first $d_{1}$ variables are observed and that the next $d_{2}$ variables are missing, i.e.,

\[
\ell(\mathbf{y}_i; \boldsymbol{\mu}, \boldsymbol{\Sigma}) = 
-\frac{1}{2} \left[ d \log(2\pi) + \log|\boldsymbol{\Sigma}| + (\mathbf{y}_i - \boldsymbol{\mu})^\top \boldsymbol{\Sigma}^{-1} (\mathbf{y}_i - \boldsymbol{\mu}) \right]
\]

## Implement a function loglik which given the data and the parameter values μ and Σ returns the log-likelihood values.
## The parameters need to be vectorized for the general purpose optimizer. Suggest a suitable vectorizing scheme.

```{r, echo=TRUE}
loglik <- function(y, mu, sigma) {
  d <- ncol(y)
  n <- nrow(y)
  log_likelihood <- numeric(n)
  
  for (i in 1:n) {
    y_i <- y[i, ]
    log_likelihood[i] <- -0.5 * (d * log(2 * pi) + log(det(sigma)) +
                                 t(y_i - mu) %*% solve(sigma) %*% (y_i - mu))
  }
  return(sum(log_likelihood))
}

# Create a wrapper to unpack parameters
neg_loglik_wrapper <- function(params, y, d) {
  mu <- params[1:d]
  L_vals <- params[(d + 1):length(params)]
  
  # Reconstruct lower triangle matrix
  L <- matrix(0, d, d)
  L[lower.tri(L, diag = TRUE)] <- L_vals
  
  # sigma = L %*% t(L), to ensure positive-definite
  sigma <- L %*% t(L)
  
  # Return negative log-likelihood (since optim minimizes)
  return(-loglik(y, mu, sigma))
}

# Generate sample data
set.seed(1234)
n <- 100
d <- 3
true_mu <- c(0, 0, 0)
true_sigma <- matrix(c(1, 0.5, 0.3,
                       0.5, 1, 0.2,
                       0.3, 0.2, 1), nrow = d)
y <- rmvnorm(n, mean = true_mu, sigma = true_sigma)

# Initial guesses
init_mu <- rep(0, d)
init_L_vals <- rep(0.1, d * (d + 1) / 2) # lower triangle of Cholesky
init_params <- c(init_mu, init_L_vals)

# Optimize
result <- optim(init_params, neg_loglik_wrapper, y = y, d = d, method = "BFGS")

# Extract optimized mu and sigma
opt_mu <- result$par[1:d]
opt_L_vals <- result$par[(d + 1):length(result$par)]
L <- matrix(0, d, d)
L[lower.tri(L, diag = TRUE)] <- opt_L_vals
opt_sigma <- L %*% t(L)

# Show result
cat("Optimized mu:\n")
print(opt_mu)
cat("Optimized sigma:\n")
print(opt_sigma)


```

## Use optim to maximize the log-likelihood on an artificial data set of size 200 drawn from a 3-dimensional multivariate normal distribution with mean zero and variance-covariance matrix

```{r, echo=TRUE}

library(mvrnorm)
# Fitting a multivariate normal distribution to the data without missing values
# Generate another sample data
set.seed(1234)
n <- 200
d <- 3
true_mu <- c(0, 0, 0)
true_sigma <- matrix(c(1, 0.5, 0.5,
             0.5, 1, 0.5,
             0.5, 0.5, 1), nrow = d)
y <- rmvnorm(n, mean = true_mu, sigma = true_sigma)

# Assign 30% missing values to each variable
missing_indices <- matrix(runif(n * d) < 0.3, nrow = n, ncol = d)
y[missing_indices] <- NA


impute_missing <- function(y, mu, sigma) {
  y_imputed <- y
  for (i in 1:nrow(y)) {
    obs_idx <- !is.na(y[i, ])
    miss_idx <- is.na(y[i, ])
    
    # Only proceed if there are both observed and missing values
    if (any(miss_idx) && any(obs_idx)) {
      mu_obs <- mu[obs_idx]
      mu_miss <- mu[miss_idx]
      sigma_oo <- sigma[obs_idx, obs_idx]
      sigma_mo <- sigma[miss_idx, obs_idx]
      y_obs <- y[i, obs_idx]
      
      # Try-catch to handle any weird numerical cases
      y_miss_hat <- tryCatch({
        mu_miss + sigma_mo %*% solve(sigma_oo) %*% (y_obs - mu_obs)
      }, error = function(e) {
        rep(NA, length(miss_idx))  # if error, don't impute
      })
      
      y_imputed[i, miss_idx] <- y_miss_hat
    }
  }
  return(y_imputed)
}

# EM algorithm

for (iter in 1:20) {
  cat("\n--- Iteration", iter, "---\n")
  
  # E-step: Impute missing values
  y_imputed <- impute_missing(y, opt_mu, opt_sigma)
  
  # M-step: Optimize log-likelihood on imputed data
  result <- optim(init_params, neg_loglik_wrapper, y = y_imputed, d = d, method = "BFGS")
  
  # Extract parameters
  opt_mu <- result$par[1:d]
  opt_L_vals <- result$par[(d + 1):length(result$par)]
  L <- matrix(0, d, d)
  L[lower.tri(L, diag = TRUE)] <- opt_L_vals
  opt_sigma <- L %*% t(L)
  
  # Update initial params for next loop
  init_params <- result$par
  
  # Print step results
  cat("mu:\n"); print(opt_mu)
  cat("sigma:\n"); print(opt_sigma)
  cat("log-likelihood (complete cases):", loglik(y, opt_mu, opt_sigma), "\n")
}




```

