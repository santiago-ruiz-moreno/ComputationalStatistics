\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand*\HyPL@Entry[1]{}
\HyPL@Entry{0<</S/D>>}
\@writefile{toc}{\contentsline {subsection}{Exercise 1}{1}{section*.1}\protected@file@percent }
\newlabel{exercise-1}{{}{1}{Exercise 1}{section*.1}{}}
\@writefile{toc}{\contentsline {section}{Exercise 2}{2}{section*.2}\protected@file@percent }
\newlabel{exercise-2}{{}{2}{Exercise 2}{section*.2}{}}
\@writefile{toc}{\contentsline {subsection}{(i) Log‑likelihood}{2}{section*.3}\protected@file@percent }
\newlabel{i-loglikelihood}{{}{2}{(i) Log‑likelihood}{section*.3}{}}
\@writefile{toc}{\contentsline {subsection}{(ii) R function for the log‑likelihood}{2}{section*.4}\protected@file@percent }
\newlabel{ii-r-function-for-the-loglikelihood}{{}{2}{(ii) R function for the log‑likelihood}{section*.4}{}}
\@writefile{toc}{\contentsline {subsection}{(iii) Maximum‐likelihood estimation via \texttt  {optim()}}{2}{section*.5}\protected@file@percent }
\newlabel{iii-maximumlikelihood-estimation-via-optim}{{}{2}{\texorpdfstring {(iii) Maximum‐likelihood estimation via \texttt {optim()}}{(iii) Maximum‐likelihood estimation via optim()}}{section*.5}{}}
\@writefile{toc}{\contentsline {section}{Exercise 3}{3}{section*.6}\protected@file@percent }
\newlabel{exercise-3}{{}{3}{Exercise 3}{section*.6}{}}
\@writefile{toc}{\contentsline {subsection}{(i) Golden‐section search function}{3}{section*.7}\protected@file@percent }
\newlabel{i-goldensection-search-function}{{}{3}{(i) Golden‐section search function}{section*.7}{}}
\@writefile{toc}{\contentsline {subsection}{(ii) Application to the light‑bulb log‑likelihood}{3}{section*.8}\protected@file@percent }
\newlabel{ii-application-to-the-lightbulb-loglikelihood}{{}{3}{(ii) Application to the light‑bulb log‑likelihood}{section*.8}{}}
\@writefile{toc}{\contentsline {section}{Exercise 4}{4}{section*.9}\protected@file@percent }
\newlabel{exercise-4}{{}{4}{Exercise 4}{section*.9}{}}
\@writefile{toc}{\contentsline {subsection}{i) Determine the complete-data log-likelihood.}{4}{section*.10}\protected@file@percent }
\newlabel{i-determine-the-complete-data-log-likelihood.}{{}{4}{i) Determine the complete-data log-likelihood}{section*.10}{}}
\@writefile{toc}{\contentsline {subsection}{ii) Determine the conditional means}{4}{section*.11}\protected@file@percent }
\newlabel{ii-determine-the-conditional-means}{{}{4}{ii) Determine the conditional means}{section*.11}{}}
\@writefile{toc}{\contentsline {subsection}{iii) Determine the E- and M-step of the EM algorithm.}{5}{section*.12}\protected@file@percent }
\newlabel{iii-determine-the-e--and-m-step-of-the-em-algorithm.}{{}{5}{iii) Determine the E- and M-step of the EM algorithm}{section*.12}{}}
\@writefile{toc}{\contentsline {subsection}{iv) Implement the EM algorithm and apply it to artificial data generated with:}{6}{section*.13}\protected@file@percent }
\newlabel{iv-implement-the-em-algorithm-and-apply-it-to-artificial-data-generated-with}{{}{6}{iv) Implement the EM algorithm and apply it to artificial data generated with:}{section*.13}{}}
\@writefile{toc}{\contentsline {section}{Exercise 5}{7}{section*.14}\protected@file@percent }
\newlabel{exercise-5}{{}{7}{Exercise 5}{section*.14}{}}
\@writefile{toc}{\contentsline {subsection}{i) Specify the log-likelihood for a single observation \(y_{i}\). Assume for \(y_{i}\) that the first \(d_{1}\) variables are observed and that the next \(d_{2}\) variables are missing, i.e.,}{8}{section*.15}\protected@file@percent }
\newlabel{i-specify-the-log-likelihood-for-a-single-observation-y_i.-assume-for-y_i-that-the-first-d_1-variables-are-observed-and-that-the-next-d_2-variables-are-missing-i.e.}{{}{8}{\texorpdfstring {i) Specify the log-likelihood for a single observation \(y_{i}\). Assume for \(y_{i}\) that the first \(d_{1}\) variables are observed and that the next \(d_{2}\) variables are missing, i.e.,}{i) Specify the log-likelihood for a single observation y\_\{i\}. Assume for y\_\{i\} that the first d\_\{1\} variables are observed and that the next d\_\{2\} variables are missing, i.e.,}}{section*.15}{}}
\@writefile{toc}{\contentsline {subsection}{ii) Implement a function loglik which given the data and the parameter values μ and Σ returns the log-likelihood values.}{8}{section*.16}\protected@file@percent }
\newlabel{ii-implement-a-function-loglik-which-given-the-data-and-the-parameter-values-ux3bc-and-ux3c3-returns-the-log-likelihood-values.}{{}{8}{ii) Implement a function loglik which given the data and the parameter values μ and Σ returns the log-likelihood values}{section*.16}{}}
\@writefile{toc}{\contentsline {subsection}{iii) The parameters need to be vectorized for the general purpose optimizer. Suggest a suitable vectorizing scheme.}{8}{section*.17}\protected@file@percent }
\newlabel{iii-the-parameters-need-to-be-vectorized-for-the-general-purpose-optimizer.-suggest-a-suitable-vectorizing-scheme.}{{}{8}{iii) The parameters need to be vectorized for the general purpose optimizer. Suggest a suitable vectorizing scheme}{section*.17}{}}
\@writefile{toc}{\contentsline {subsection}{iv) Use optim to maximize the log-likelihood on an artificial data set of size 200 drawn from a 3-dimensional multivariate normal distribution with mean zero and variance-covariance matrix}{10}{section*.18}\protected@file@percent }
\newlabel{iv-use-optim-to-maximize-the-log-likelihood-on-an-artificial-data-set-of-size-200-drawn-from-a-3-dimensional-multivariate-normal-distribution-with-mean-zero-and-variance-covariance-matrix}{{}{10}{iv) Use optim to maximize the log-likelihood on an artificial data set of size 200 drawn from a 3-dimensional multivariate normal distribution with mean zero and variance-covariance matrix}{section*.18}{}}
\gdef \@abspage@last{13}
